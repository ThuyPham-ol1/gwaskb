{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenotype/SNP relation extraction from tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will demo the module that parses tables in papers and extracts relations between SNPs and phenotypes (in cases in which the paper discusses multiple phenotypes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by configuring Jupyter and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "\n",
    "# set the paths to snorkel and gwaskb\n",
    "sys.path.append('../snorkel-tables')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/crawler')\n",
    "\n",
    "# set up the directory with the input papers\n",
    "abstract_dir = '../data/db/papers'\n",
    "\n",
    "# set up matplotlib\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "\n",
    "# create a Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our usual corpus of GWAS papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from extractor.parser import UnicodeXMLTableDocParser\n",
    "from snorkel.parser import XMLMultiDocParser\n",
    "\n",
    "xml_parser = XMLMultiDocParser(\n",
    "    path=abstract_dir,\n",
    "    doc='./*',\n",
    "    text='.//table',\n",
    "    id='.//article-id[@pub-id-type=\"pmid\"]/text()',\n",
    "    keep_xml_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus of 589 documents\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser, OmniParser\n",
    "from snorkel.models import Corpus\n",
    "\n",
    "# parses tables into rows, cols, cells...\n",
    "table_parser = OmniParser(timeout=1000000)\n",
    "\n",
    "try:\n",
    "    corpus = session.query(Corpus).filter(Corpus.name == 'GWAS Table Corpus').one()\n",
    "except:\n",
    "    cp = CorpusParser(xml_parser, table_parser)\n",
    "    %time corpus = cp.parse_corpus(name='GWAS Table Corpus', session=session)\n",
    "    session.add(corpus)\n",
    "    session.commit()\n",
    "\n",
    "print 'Loaded corpus of %d documents' % len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define candidate matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RSid matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import RegexMatchSpan\n",
    "rsid_matcher = RegexMatchSpan(rgx=r'rs\\d+(/[ATCG]{1,2})*$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phenotype matchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first matcher checks if we are in a column whose header labels it as a phenotype column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import CellNameDictionaryMatcher\n",
    "\n",
    "phen_words = ['trait', 'phenotype', 'outcome'] # words that denote phenotypes\n",
    "phen_matcher = CellNameDictionaryMatcher(axis='col', d=phen_words, n_max=3, ignore_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next matcher will match phenotypes in cells that span an entire axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch\n",
    "from db.kb import KnowledgeBase\n",
    "from extractor.util import make_ngrams\n",
    "\n",
    "# collect phenotype list\n",
    "kb = KnowledgeBase()\n",
    "# efo phenotypes\n",
    "efo_phenotype_list0 = kb.get_phenotype_candidates(source='efo', peek=True) # TODO: remove peaking\n",
    "efo_phenotype_list = list(make_ngrams(efo_phenotype_list0))\n",
    "# mesh diseases\n",
    "mesh_phenotype_list0 = kb.get_phenotype_candidates(source='mesh')\n",
    "mesh_phenotype_list = list(make_ngrams(mesh_phenotype_list0))\n",
    "# mesh chemicals\n",
    "chem_phenotype_list = kb.get_phenotype_candidates(source='chemical')\n",
    "\n",
    "phenotype_names = efo_phenotype_list + mesh_phenotype_list + chem_phenotype_list\n",
    "phen_name_matcher = DictionaryMatch(d=phenotype_names, ignore_case=True, stemmer='porter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Relation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "from snorkel.throttlers import AlignmentThrottler, SeparatingSpanThrottler, OrderingThrottler, CombinedThrottler\n",
    "\n",
    "# create a Snorkel class for the relation we will extract\n",
    "from snorkel.models import candidate_subclass\n",
    "RsidPhenRel = candidate_subclass('RsidPhenRel', ['rsid','phen'])\n",
    "\n",
    "# define our candidate spaces\n",
    "from snorkel.candidates import TableNgrams, TableCells, SpanningTableCells\n",
    "unigrams = TableNgrams(n_max=1)\n",
    "cells = TableCells()\n",
    "spanning_cells = SpanningTableCells(axis='row')\n",
    "\n",
    "# we will be looking only at aligned cells\n",
    "row_align_filter = AlignmentThrottler(axis='row', infer=True)\n",
    "\n",
    "# and at cells where the phenotype is in a spanning header cell above the rsid cell\n",
    "sep_span_filter = SeparatingSpanThrottler(align_axis='col') # rsid and phen are not separated by spanning cells\n",
    "col_order_filter = OrderingThrottler(axis='col', first=1) # phen spanning cell comes first\n",
    "header_filter = CombinedThrottler([sep_span_filter, col_order_filter]) # combine the two throttlers\n",
    "\n",
    "# the first extractor looks at phenotype names in columns with a header indicating it's a phenotype\n",
    "ce1 = CandidateExtractor(RsidPhenRel, [unigrams, cells], [rsid_matcher, phen_matcher], throttler=row_align_filter)\n",
    "\n",
    "# the second extractor looks at phenotype names in columns with a header indicating it's a phenotype\n",
    "ce2 = CandidateExtractor(RsidPhenRel, [unigrams, spanning_cells], [rsid_matcher, phen_name_matcher], throttler=header_filter, stop_on_duplicates=False)\n",
    "\n",
    "# collect that cells that will be searched for candidates\n",
    "tables = [table for doc in corpus.documents for table in doc.tables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform relation extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3646 relations extracted, e.g.\n",
      "RsidPhenRel(Span(\"rs464766\", parent=302832, chars=[0,7], words=[0,0]), Span(\"Mean BMI\", parent=302831, chars=[0,7], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs10504576\", parent=302729, chars=[0,9], words=[0,0]), Span(\"Mean WC\", parent=302728, chars=[0,6], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs2296465\", parent=302754, chars=[0,8], words=[0,0]), Span(\"Mean BMI\", parent=302753, chars=[0,7], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs315711\", parent=302948, chars=[0,7], words=[0,0]), Span(\"Mean WC\", parent=302947, chars=[0,6], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs2221880\", parent=302780, chars=[0,8], words=[0,0]), Span(\"Mean BMI\", parent=302779, chars=[0,7], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs10488165\", parent=302873, chars=[0,9], words=[0,0]), Span(\"Mean WC\", parent=302872, chars=[0,6], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs4129319\", parent=302812, chars=[0,8], words=[0,0]), Span(\"Mean WC\", parent=302811, chars=[0,6], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs1374489\", parent=302760, chars=[0,8], words=[0,0]), Span(\"Mean BMI\", parent=302759, chars=[0,7], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs7941883\", parent=302908, chars=[0,8], words=[0,0]), Span(\"Mean WC\", parent=302907, chars=[0,6], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs7202384\", parent=302858, chars=[0,8], words=[0,0]), Span(\"Mean BMI\", parent=302857, chars=[0,7], words=[0,1]))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    rels1 = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Set 1').one()\n",
    "except:\n",
    "    %time rels1 = ce1.extract(tables, 'RsidPhenRel Set 1', session)\n",
    "    \n",
    "print \"%s relations extracted, e.g.\" % len(rels1)\n",
    "for cand in rels1[:10]:\n",
    "    print cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 relations extracted, e.g.\n",
      "RsidPhenRel(Span(\"rs6462411\", parent=320685, chars=[0,8], words=[0,0]), Span(\"Thyroid Stimulating Hormone\", parent=320670, chars=[0,26], words=[0,2]))\n",
      "RsidPhenRel(Span(\"rs10848704\", parent=320706, chars=[0,9], words=[0,0]), Span(\"Thyroid Stimulating Hormone\", parent=320670, chars=[0,26], words=[0,2]))\n",
      "RsidPhenRel(Span(\"rs925488\", parent=320749, chars=[0,7], words=[0,0]), Span(\"Thyroid Stimulating Hormone\", parent=320670, chars=[0,26], words=[0,2]))\n",
      "RsidPhenRel(Span(\"rs7804166\", parent=320791, chars=[0,8], words=[0,0]), Span(\"Thyroid Stimulating Hormone\", parent=320670, chars=[0,26], words=[0,2]))\n",
      "RsidPhenRel(Span(\"rs6956479\", parent=320909, chars=[0,8], words=[0,0]), Span(\"Thyroid Stimulating Hormone\", parent=320670, chars=[0,26], words=[0,2]))\n",
      "RsidPhenRel(Span(\"rs976731\", parent=320220, chars=[0,7], words=[0,0]), Span(\"Waist Circumference\", parent=320185, chars=[0,18], words=[0,1]))\n",
      "RsidPhenRel(Span(\"rs2225614\", parent=320241, chars=[0,8], words=[0,0]), Span(\"Weight\", parent=320237, chars=[0,5], words=[0,0]))\n",
      "RsidPhenRel(Span(\"rs2805810\", parent=320898, chars=[0,8], words=[0,0]), Span(\"Thyroid Stimulating Hormone\", parent=320670, chars=[0,26], words=[0,2]))\n",
      "RsidPhenRel(Span(\"rs6560749\", parent=320156, chars=[0,8], words=[0,0]), Span(\"BMI\", parent=320152, chars=[0,2], words=[0,0]))\n",
      "RsidPhenRel(Span(\"rs10245389\", parent=320962, chars=[0,9], words=[0,0]), Span(\"Thyroid Stimulating Hormone\", parent=320670, chars=[0,26], words=[0,2]))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    rels2 = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Set 2').one()\n",
    "except:\n",
    "    %time rels2 = ce2.extract(tables, 'RsidPhenRel Set 2', session)\n",
    "    \n",
    "print \"%s relations extracted, e.g.\" % len(rels2)\n",
    "for cand in rels2[:10]: \n",
    "    print cand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the two sets of candiates into a single set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3726 candidates in total\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    rels = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Canidates').one()\n",
    "except:\n",
    "    rels = CandidateSet(name='RsidPhenRel Canidates')\n",
    "    for c in rels1: rels.append(c)\n",
    "    for c in rels2: rels.append(c)\n",
    "\n",
    "    session.add(rels)\n",
    "    session.commit()\n",
    "\n",
    "print '%d candidates in total' % len(rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning the correctness of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train machine learning models to identify which phenotype candidates are actually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a labeled set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split data into an (unlabeled) training set (since we will use unsupervised risk estimation to train a candidate on it), and a dev/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 1863 training and 1863 dev/testing candidates\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_c = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Training Candidates').one()\n",
    "    devtest_c = session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Dev/Test Candidates').one()\n",
    "except:\n",
    "    # delete any previous sets with that name\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Training Candidates').delete()\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'RsidPhenRel Dev/Test Candidates').delete()\n",
    "\n",
    "    frac_test = 0.5\n",
    "\n",
    "    # initialize the new sets\n",
    "    train_c = CandidateSet(name='RsidPhenRel Training Candidates')\n",
    "    devtest_c = CandidateSet(name='RsidPhenRel Dev/Test Candidates')\n",
    "\n",
    "    # choose a random subset for the labeled set\n",
    "    n_test = len(rels) * frac_test\n",
    "    test_idx = set(np.random.choice(len(rels), size=(n_test,), replace=False))\n",
    "\n",
    "    # add to the sets\n",
    "    for i, c in enumerate(rels):\n",
    "        if i in test_idx:\n",
    "            devtest_c.append(c)\n",
    "        else:\n",
    "            train_c.append(c)\n",
    "\n",
    "    # save the results\n",
    "    session.add(train_c)\n",
    "    session.add(devtest_c)\n",
    "    session.commit()\n",
    "\n",
    "print 'Initialized %d training and %d dev/testing candidates' % (len(train_c), len(devtest_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Labelling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the data programming approach, we define set of labeling functions. We will learn their accuracy via unsupervised learning and use them for classifying candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 162 of the file /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "s=None\n",
    "doc = [d for d in corpus.documents if d.name == '17903303'][0]\n",
    "table = doc.tables[3]\n",
    "for cell in table.cells:\n",
    "    top_cells = get_aligned_cells(cell, 'col', infer=True)\n",
    "    top_phrases = [phrase for cell in top_cells for phrase in cell.phrases]\n",
    "# rels[0][1].parent.table.cells[0].phrases\n",
    "# corpus.documents[0].phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import *\n",
    "\n",
    "bad_words = ['rs number', 'rs id', 'rsid']\n",
    "\n",
    "# negative LFs\n",
    "def LF_number(m):\n",
    "    txt = m[1].get_span()\n",
    "    frac_num = len([ch for ch in txt if ch.isdigit()]) / float(len(txt))\n",
    "    return -1 if len(txt) > 5 and frac_num > 0.4 or frac_num > 0.6 else 0\n",
    "\n",
    "def LF_bad_phen_mentions(m):\n",
    "    if cell_spans(m[1].parent.cell, m[1].parent.table, 'row'): return 0\n",
    "    #     if m[1].context.cell.spans('row'): return 0\n",
    "    top_cells = get_aligned_cells(m[1].parent.cell, 'col', infer=True)\n",
    "    top_cells = [cell for cell in top_cells]\n",
    "#     top_cells = m.span1.context.cell.aligned_cells(axis='col', induced=True)\n",
    "    try:\n",
    "        top_phrases = [phrase for cell in top_cells for phrase in cell.phrases]\n",
    "    except:\n",
    "        for cell in top_cells:\n",
    "            print cell, cell.phrases\n",
    "    if not top_phrases: return 0\n",
    "    matching_phrases = []\n",
    "    for phrase in top_phrases:\n",
    "        if any (phen_matcher._f_ngram(word) for word in phrase.text.split(' ')):\n",
    "            matching_phrases.append(phrase)\n",
    "    small_matching_phrases = [phrase for phrase in matching_phrases if len(phrase.text) <= 25]\n",
    "    return -1 if not small_matching_phrases else 0\n",
    "\n",
    "def LF_bad_word(m):\n",
    "    txt = m[1].get_span()\n",
    "    return -1 if any(word in txt for word in bad_words) else 0\n",
    "\n",
    "LF_tables_neg = [LF_number, LF_bad_phen_mentions]\n",
    "\n",
    "# positive LFs\n",
    "def LF_no_neg(m):\n",
    "    return +1 if not any(LF(m) for LF in LF_tables_neg) else 0\n",
    "\n",
    "LF_tables_pos = [LF_no_neg]\n",
    "\n",
    "LFs = LF_tables_neg + LF_tables_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate features for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.4 ms, sys: 9.73 ms, total: 106 ms\n",
      "Wall time: 133 ms\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "try:\n",
    "    %time L_train = label_manager.load(session, train_c, 'RsidPhenRel LF Labels6')\n",
    "except sqlalchemy.orm.exc.NoResultFound:\n",
    "    %time L_train = label_manager.create(session, train_c, 'RsidPhenRel LF Labels6', f=LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at some basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conflicts</th>\n",
       "      <th>coverage</th>\n",
       "      <th>j</th>\n",
       "      <th>overlaps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_number</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0.129361</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.065486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_bad_phen_mentions</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0.139560</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0.065486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_no_neg</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0.796565</td>\n",
       "      <td> 2</td>\n",
       "      <td> 0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      conflicts  coverage  j  overlaps\n",
       "LF_number                     0  0.129361  0  0.065486\n",
       "LF_bad_phen_mentions          0  0.139560  1  0.065486\n",
       "LF_no_neg                     0  0.796565  2  0.000000\n",
       "\n",
       "[3 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a generative model, just like in the phenotype extraction notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t1863\n",
      "Features:\t\t\t3\n",
      "================================================================================\n",
      "Begin training for rate=0.01, mu=1e-06\n",
      "\tLearning epoch = 0\tGradient mag. = 0.069766\n",
      "\tLearning epoch = 250\tGradient mag. = 0.079974\n",
      "\tLearning epoch = 500\tGradient mag. = 0.086107\n",
      "\tLearning epoch = 750\tGradient mag. = 0.091676\n",
      "\tLearning epoch = 1000\tGradient mag. = 0.096584\n",
      "\tLearning epoch = 1250\tGradient mag. = 0.100796\n",
      "\tLearning epoch = 1500\tGradient mag. = 0.104331\n",
      "\tLearning epoch = 1750\tGradient mag. = 0.107247\n",
      "\tLearning epoch = 2000\tGradient mag. = 0.109621\n",
      "\tLearning epoch = 2250\tGradient mag. = 0.111536\n",
      "\tLearning epoch = 2500\tGradient mag. = 0.113072\n",
      "\tLearning epoch = 2750\tGradient mag. = 0.114301\n",
      "\tLearning epoch = 3000\tGradient mag. = 0.115283\n",
      "\tLearning epoch = 3250\tGradient mag. = 0.116066\n",
      "\tLearning epoch = 3500\tGradient mag. = 0.116693\n",
      "\tLearning epoch = 3750\tGradient mag. = 0.117194\n",
      "\tLearning epoch = 4000\tGradient mag. = 0.117596\n",
      "\tLearning epoch = 4250\tGradient mag. = 0.117918\n",
      "\tLearning epoch = 4500\tGradient mag. = 0.118178\n",
      "\tLearning epoch = 4750\tGradient mag. = 0.118386\n",
      "\tLearning epoch = 5000\tGradient mag. = 0.118554\n",
      "\tLearning epoch = 5250\tGradient mag. = 0.118690\n",
      "\tLearning epoch = 5500\tGradient mag. = 0.118800\n",
      "\tLearning epoch = 5750\tGradient mag. = 0.118888\n",
      "\tLearning epoch = 6000\tGradient mag. = 0.118960\n",
      "\tLearning epoch = 6250\tGradient mag. = 0.119017\n",
      "\tLearning epoch = 6500\tGradient mag. = 0.119064\n",
      "\tLearning epoch = 6750\tGradient mag. = 0.119102\n",
      "\tLearning epoch = 7000\tGradient mag. = 0.119133\n",
      "\tLearning epoch = 7250\tGradient mag. = 0.119158\n",
      "\tLearning epoch = 7500\tGradient mag. = 0.119178\n",
      "\tLearning epoch = 7750\tGradient mag. = 0.119194\n",
      "\tLearning epoch = 8000\tGradient mag. = 0.119208\n",
      "\tLearning epoch = 8250\tGradient mag. = 0.119219\n",
      "\tLearning epoch = 8500\tGradient mag. = 0.119227\n",
      "\tLearning epoch = 8750\tGradient mag. = 0.119234\n",
      "\tLearning epoch = 9000\tGradient mag. = 0.119240\n",
      "\tLearning epoch = 9250\tGradient mag. = 0.119245\n",
      "\tLearning epoch = 9500\tGradient mag. = 0.119248\n",
      "\tLearning epoch = 9750\tGradient mag. = 0.119252\n",
      "Final gradient magnitude for rate=0.01, mu=1e-06: 0.119\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model = NaiveBayes()\n",
    "gen_model.train(L_train, n_iter=10000, rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.08323015,  8.86630481,  0.98503744])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify all the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating annotations for 3726 candidates...\n",
      "[========================================] 100%\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 14min 4s, sys: 4min 3s, total: 18min 7s\n",
      "Wall time: 18min 13s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "# delete existing labels\n",
    "# session.rollback()\n",
    "# session.query(AnnotationKeySet).filter(AnnotationKeySet.name == 'RsidPhenRel LF All Labels').delete()\n",
    "%time L_all = label_manager.create(session, rels, 'RsidPhenRel LF All Lab', f=LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2958 relations extracted, e.g.:\n",
      "[(u'17903300', u'rs464766', u'Mean BMI'), (u'17903300', u'rs10504576', u'Mean WC'), (u'17903300', u'rs2296465', u'Mean BMI'), (u'17903300', u'rs315711', u'Mean WC'), (u'17903300', u'rs2221880', u'Mean BMI'), (u'17903300', u'rs10488165', u'Mean WC'), (u'17903300', u'rs4129319', u'Mean WC'), (u'17903300', u'rs1374489', u'Mean BMI'), (u'17903300', u'rs7941883', u'Mean WC'), (u'17903300', u'rs7202384', u'Mean BMI')]\n"
     ]
    }
   ],
   "source": [
    "preds = gen_model.odds(L_all)\n",
    "good_rels = [(c[0].parent.document.name, c[0].get_span(), c[1].get_span()) for (c, p) in zip(rels, preds) if p > 0]\n",
    "print len(good_rels), 'relations extracted, e.g.:'\n",
    "print good_rels[:10]\n",
    "\n",
    "# store relations to annotate\n",
    "with open('results/nb-output/rels.acronyms.extracted.tsv', 'w') as f:\n",
    "    for doc_id, str1, str2 in good_rels:\n",
    "        try:\n",
    "            out = u'{}\\t{}\\t{}\\n'.format(doc_id, unicode(str1), str2)\n",
    "            f.write(out.encode(\"UTF-8\"))\n",
    "        except:\n",
    "            print 'Error saving:', str1, str2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acronym resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large fraction of the phenotypes we extracted consist of acronyms. This section deals with translating these acronyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step requires a list of acronyms and the phenotype to which they correspond. You need to run the acronym extraction notebook to produce this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3221 definitions loaded\n"
     ]
    }
   ],
   "source": [
    "from extractor.dictionary import Dictionary, unravel\n",
    "\n",
    "D = Dictionary()\n",
    "D.load('results/nb-output/acronyms.extracted.all.tsv')\n",
    "print len(D), 'definitions loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dictionary object performs acronym translation. We apply it to our extracted relations to produce a new, translated list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filter relations with low p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of the above relations will involve SNPs that are not statistically significant. Here, we would like to prefilter them based on p-values extracted in the p-value extraction notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't yet generated these p-values, you may skip this step (just comment out the filtering in the final cell), as we will perform filtering in the final notebook anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pval_rsid_dict = dict()\n",
    "pval_dict = dict() # combine all of the pvalues for a SNPs in the same document into one set\n",
    "with open('results/nb-output/pval-rsid.tsv') as f:\n",
    "    for line in f:\n",
    "        pmid, rsid, table_id, row_id, col_id, log_pval = line.strip().split('\\t')\n",
    "        log_pval, table_id, row_id, col_id = float(log_pval), int(table_id), int(row_id), int(col_id)\n",
    "        \n",
    "        if pmid not in pval_rsid_dict: pval_rsid_dict[pmid] = dict()\n",
    "        key = (rsid, table_id, row_id)\n",
    "        if key not in pval_rsid_dict[pmid]: pval_rsid_dict[pmid][key] = set()\n",
    "        pval_rsid_dict[pmid][key].add(log_pval)\n",
    "                \n",
    "        if pmid not in pval_dict: pval_dict[pmid] = dict()\n",
    "        if rsid not in pval_dict[pmid]: pval_dict[pmid][rsid] = set()\n",
    "        pval_dict[pmid][rsid].add(log_pval)\n",
    "\n",
    "pval_dict0 = {pmid : {rsid : min(pval_dict[pmid][rsid]) for rsid in pval_dict[pmid]} for pmid in pval_dict}\n",
    "pval_rsid_dict0 = {pmid : {key : min(pval_rsid_dict[pmid][key]) for key in pval_rsid_dict[pmid]} for pmid in pval_rsid_dict}\n",
    "pval_dict = pval_dict0\n",
    "pval_rsid_dict = pval_rsid_dict0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all relations that have sufficiently small p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we resolve acronyms and save the relations with their resolved phenotype names. We also store their coordinates (table, row, col) in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preds = learner.predict_wmv(candidates)\n",
    "predicted_candidates =  [c for (c, p) in zip(rels, preds) if p > 0]\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "def _normalize_str(s):\n",
    "    try:\n",
    "        s = s.encode('utf-8')\n",
    "        return s\n",
    "    except UnicodeEncodeError: \n",
    "        pass\n",
    "    try:\n",
    "        s = s.decode('utf-8')\n",
    "        return s\n",
    "    except UnicodeDecodeError: \n",
    "        pass    \n",
    "    raise Exception()\n",
    "    \n",
    "def clean_rsid(rsid):\n",
    "    return re.sub('/.+', '', rsid)\n",
    "\n",
    "with open('results/nb-output/phen-rsid.table.rel.all.tsv', 'w') as f:\n",
    "    for c in predicted_candidates:\n",
    "        pmid = c[0].parent.document.name\n",
    "        rsid = c[0].get_span()\n",
    "        phen = c[1].get_span()        \n",
    "        table_id = c[0].parent.table.position\n",
    "        row_num = c[0].parent.cell.row.position\n",
    "        col_num = c[0].parent.cell.col.position # of the rsid\n",
    "        \n",
    "        if row_num is None:\n",
    "            print c[0].parent.cell\n",
    "\n",
    "        phen = (unravel(pmid, phen, D))\n",
    "        if isinstance(phen, unicode):\n",
    "            phen = phen.encode('utf-8')\n",
    "        \n",
    "        try:\n",
    "            log_pval = pval_rsid_dict[pmid][(rsid, table_id, row_num)]\n",
    "        except KeyError:\n",
    "            log_pval = -1000\n",
    "#             continue\n",
    "        if 10**log_pval > 1e-5: continue\n",
    "\n",
    "        out_str = '{pmid}\\t{rsid}\\t{phen}\\t{pval}\\ttable\\t{table_id}\\t{row}\\t{col}\\n'.format(\n",
    "                    pmid=pmid, rsid=clean_rsid(rsid), phen=phen, pval=log_pval, table_id=table_id, row=row_num, col=col_num)\n",
    "        f.write(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
