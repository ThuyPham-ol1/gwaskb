{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenotype acronym extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module looks at both tables and text to identify acronyms used to refer to phenotypes. These acronyms are then used to further expand SNP/phenotype relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by configuring Jupyter and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "\n",
    "# set the paths to snorkel and gwaskb\n",
    "sys.path.append('../snorkel-tables')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/crawler')\n",
    "\n",
    "# set up the directory with the input papers\n",
    "abstract_dir = '../data/db/papers'\n",
    "\n",
    "# set up matplotlib\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12,4)\n",
    "\n",
    "# create a Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our usual corpus of GWAS papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the acronyms are found in tables. We parse these like in the other table-based modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import XMLMultiDocParser\n",
    "\n",
    "xml_parser = XMLMultiDocParser(\n",
    "    path=abstract_dir,\n",
    "    doc='./*',\n",
    "    text='.//table',\n",
    "    id='.//article-id[@pub-id-type=\"pmid\"]/text()',\n",
    "    keep_xml_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus of 589 documents\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import CorpusParser, OmniParser\n",
    "from snorkel.models import Corpus\n",
    "\n",
    "# parses tables into rows, cols, cells...\n",
    "table_parser = OmniParser(timeout=1000000)\n",
    "\n",
    "try:\n",
    "    table_corpus = session.query(Corpus).filter(Corpus.name == 'GWAS Table Corpus').one()\n",
    "except:\n",
    "    cp = CorpusParser(xml_parser, table_parser)\n",
    "    %time table_corpus = cp.parse_corpus(name='GWAS Table Corpus', session=session)\n",
    "    session.add(table_corpus)\n",
    "    session.commit()\n",
    "\n",
    "print 'Loaded corpus of %d documents' % len(table_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text copus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also seek mentions of acronyms in the paper text.\n",
    "\n",
    "The following parser extracts sentences from each paper abstract, title, and the first 5 paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus of 589 documents\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import SentenceParser\n",
    "from snorkel.parser import CorpusParser\n",
    "from snorkel.models import Corpus\n",
    "\n",
    "from extractor.parser import UnicodeXMLDocParser, GWASXMLDocParser\n",
    "\n",
    "xml_parser = GWASXMLDocParser(\n",
    "    path=abstract_dir,\n",
    "    doc='./*',\n",
    "    title='.//front//article-title//text()',\n",
    "    abstract='.//abstract//p//text()',\n",
    "    n_par=5,\n",
    "    id='.//article-id[@pub-id-type=\"pmid\"]/text()',\n",
    "    keep_xml_tree=True)\n",
    "\n",
    "sent_parser = SentenceParser()\n",
    "\n",
    "try:\n",
    "    text_corpus = session.query(Corpus).filter(Corpus.name == 'GWAS Text Corpus').one()\n",
    "except:\n",
    "    cp = CorpusParser(xml_parser, sent_parser)\n",
    "    %time text_corpus = cp.parse_corpus(name='GWAS Text Corpus', session=session)\n",
    "    session.add(text_corpus)\n",
    "    session.commit()\n",
    "\n",
    "print 'Loaded corpus of %d documents' % len(text_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate candidates from both tables and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### From phenotype / acronym tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many papers have tables with an acronym column and a phenotype column. In this section, we extract candidates from these tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define matchers for cells whose header contains a word that is indicative of a phenotype or an acronym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a Snorkel class for the relation we will extract\n",
    "from snorkel.models import candidate_subclass\n",
    "AcroPhenRel = candidate_subclass('AcroPhenRel', ['acro','phen'])\n",
    "\n",
    "# Define a candidate space\n",
    "from snorkel.candidates import TableCells\n",
    "cells = TableCells()\n",
    "\n",
    "# Create a list of possible words that could denote phenotypes\n",
    "acro_words = ['abbreviation', 'acronym', 'phenotype']\n",
    "phen_words = ['trait', 'phenotype', 'description']\n",
    "\n",
    "# Define matchers\n",
    "from snorkel.matchers import CellNameDictionaryMatcher\n",
    "phen_matcher = CellNameDictionaryMatcher(axis='col', d=phen_words, n_max=3, ignore_case=True)\n",
    "acro_matcher = CellNameDictionaryMatcher(axis='col', d=acro_words, n_max=3, ignore_case=True)\n",
    "\n",
    "# we will be looking only at aligned cells\n",
    "from snorkel.throttlers import AlignmentThrottler\n",
    "row_align_filter = AlignmentThrottler(axis='row', infer=True)\n",
    "\n",
    "# create the candidate extractor\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "ce1 = CandidateExtractor(AcroPhenRel, [cells, cells], [acro_matcher, phen_matcher], throttler=row_align_filter)\n",
    "\n",
    "# collect that cells that will be searched for candidates\n",
    "tables = [table for doc in table_corpus.documents for table in doc.tables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform relation extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 relations extracted, e.g.\n",
      "AcroPhenRel(Span(\"MP:0000284\", parent=477868, chars=[0,9], words=[0,1]), Span(\"double outlet right ventricle\", parent=477869, chars=[0,28], words=[0,3]))\n",
      "AcroPhenRel(Span(\"heart right ventricle hypertrophy\", parent=477885, chars=[0,32], words=[0,3]), Span(\"MP:0000276\", parent=477884, chars=[0,9], words=[0,1]))\n",
      "AcroPhenRel(Span(\"abnormal heart right atrium morphology\", parent=477829, chars=[0,37], words=[0,4]), Span(\"MP:0003922\", parent=477828, chars=[0,9], words=[0,1]))\n",
      "AcroPhenRel(Span(\"MP:0002625\", parent=477876, chars=[0,9], words=[0,1]), Span(\"heart left ventricle hypertrophy\", parent=477877, chars=[0,31], words=[0,3]))\n",
      "AcroPhenRel(Span(\"right pulmonary isomerism\", parent=477917, chars=[0,24], words=[0,2]), Span(\"MP:0000531\", parent=477916, chars=[0,9], words=[0,1]))\n",
      "AcroPhenRel(Span(\"MP:0009570\", parent=477852, chars=[0,9], words=[0,1]), Span(\"abnormal right lung morphology\", parent=477853, chars=[0,29], words=[0,3]))\n",
      "AcroPhenRel(Span(\"MP:0002766\", parent=477932, chars=[0,9], words=[0,1]), Span(\"situs inversus\", parent=477933, chars=[0,13], words=[0,1]))\n",
      "AcroPhenRel(Span(\"MP:0010429\", parent=477820, chars=[0,9], words=[0,1]), Span(\"abnormal heart left ventricle outflow tract morphology\", parent=477821, chars=[0,53], words=[0,6]))\n",
      "AcroPhenRel(Span(\"abnormal left-right axis patterning\", parent=477845, chars=[0,34], words=[0,3]), Span(\"MP:0001706\", parent=477844, chars=[0,9], words=[0,1]))\n",
      "AcroPhenRel(Span(\"abnormal left lung morphology\", parent=477837, chars=[0,28], words=[0,3]), Span(\"MP:0009569\", parent=477836, chars=[0,9], words=[0,1]))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    tab_rels = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Tables Set').one()\n",
    "except:\n",
    "    %time tab_rels = ce1.extract(tables, 'AcroPhenRel Tables Set', session)\n",
    "    \n",
    "print \"%s relations extracted, e.g.\" % len(tab_rels)\n",
    "for cand in tab_rels[:10]:\n",
    "    print cand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From table phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of definining acronyms is in text, e.g. as in \"Body Mass Index (BMI)\". We are now going to extract such candidates from phrases that are found in paper tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a candidate space\n",
    "from snorkel.candidates import OmniNgrams\n",
    "ngrams3 = OmniNgrams(n_max=3)\n",
    "ngrams8 = OmniNgrams(n_max=8)\n",
    "\n",
    "# Define matchers\n",
    "from snorkel.matchers import RegexMatchSpan\n",
    "phen_matcher = RegexMatchSpan(rgx=r'.+ \\([a-zA-Z0-9_-]{1,10}[\\);]')\n",
    "acro_matcher = RegexMatchSpan(rgx=r'\\([a-zA-Z0-9_-]{1,10}[\\);]')\n",
    "\n",
    "# We only look at phenotype and acronym matches that overlap\n",
    "from snorkel.throttlers import ContainmentThrottler, WordLengthThrottler, CombinedThrottler\n",
    "containment_filter = ContainmentThrottler()\n",
    "# length_filter = WordLengthThrottler(op='max', idx=1, lim=15)\n",
    "# ovl_len_filter = CombinedThrottler([overlap_filter, length_filter])\n",
    "\n",
    "# create the candidate extractor\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "ce2 = CandidateExtractor(AcroPhenRel, [ngrams3, ngrams8], [acro_matcher, phen_matcher], self_relations=True, nested_relations=True, throttler=containment_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now extract these candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6545 relations extracted, e.g.\n",
      "AcroPhenRel(Span(\"(SE)\", parent=488275, chars=[5,8], words=[1,3]), Span(\"Beta (SE)\", parent=488275, chars=[0,8], words=[0,3]))\n",
      "AcroPhenRel(Span(\"(SE)\", parent=488282, chars=[5,8], words=[1,3]), Span(\"Beta (SE)\", parent=488282, chars=[0,8], words=[0,3]))\n",
      "AcroPhenRel(Span(\"(SE)\", parent=488277, chars=[5,8], words=[1,3]), Span(\"Beta (SE)\", parent=488277, chars=[0,8], words=[0,3]))\n",
      "AcroPhenRel(Span(\"(SNFs)\", parent=469891, chars=[12,17], words=[2,4]), Span(\"GWAS Sample (SNFs)\", parent=469891, chars=[0,17], words=[0,4]))\n",
      "AcroPhenRel(Span(\"(SNFs)\", parent=469891, chars=[12,17], words=[2,4]), Span(\"Sample (SNFs)\", parent=469891, chars=[5,17], words=[1,4]))\n",
      "AcroPhenRel(Span(\"(Unrelateds)\", parent=469892, chars=[12,23], words=[2,4]), Span(\"GWAS Sample (Unrelateds)\", parent=469892, chars=[0,23], words=[0,4]))\n",
      "AcroPhenRel(Span(\"(Unrelateds)\", parent=469892, chars=[12,23], words=[2,4]), Span(\"Sample (Unrelateds)\", parent=469892, chars=[5,23], words=[1,4]))\n",
      "AcroPhenRel(Span(\"(Unrelateds)\", parent=469893, chars=[19,30], words=[2,4]), Span(\"Sample (Unrelateds)\", parent=469893, chars=[12,30], words=[1,4]))\n",
      "AcroPhenRel(Span(\"(Unrelateds)\", parent=469893, chars=[19,30], words=[2,4]), Span(\"Replication Sample (Unrelateds)\", parent=469893, chars=[0,30], words=[0,4]))\n",
      "AcroPhenRel(Span(\"(vCJD)\", parent=317323, chars=[2,7], words=[1,3]), Span(\"p (vCJD)\", parent=317323, chars=[0,7], words=[0,3]))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    txt_tab_rels = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Tables Set 2').one()\n",
    "except:\n",
    "    %time txt_tab_rels = ce2.extract(table_corpus.documents, 'AcroPhenRel Tables Set 2', session)\n",
    "    \n",
    "print \"%s relations extracted, e.g.\" % len(txt_tab_rels)\n",
    "for cand in txt_tab_rels[:10]:\n",
    "    print cand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we repeat the same extraction process for candidates that are found in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a candidate space\n",
    "from snorkel.candidates import Ngrams\n",
    "ngrams3 = Ngrams(n_max=3)\n",
    "ngrams8 = Ngrams(n_max=8)\n",
    "\n",
    "# Define matchers\n",
    "from snorkel.matchers import RegexMatchSpan\n",
    "phen_matcher = RegexMatchSpan(rgx=r'.+ \\([a-zA-Z0-9_-]{1,10}[\\);]')\n",
    "acro_matcher = RegexMatchSpan(rgx=r'\\([a-zA-Z0-9_-]{1,10}[\\);]')\n",
    "\n",
    "# We only look at phenotype and acronym matches that overlap\n",
    "from snorkel.throttlers import ContainmentThrottler, WordLengthThrottler, CombinedThrottler\n",
    "containment_filter = ContainmentThrottler()\n",
    "# length_filter = WordLengthThrottler(op='max', idx=1, lim=15)\n",
    "# ovl_len_filter = CombinedThrottler([overlap_filter, length_filter])\n",
    "\n",
    "# create the candidate extractor\n",
    "from snorkel.candidates import CandidateExtractor\n",
    "ce3 = CandidateExtractor(AcroPhenRel, [ngrams3, ngrams8], [acro_matcher, phen_matcher], self_relations=True, nested_relations=True, throttler=containment_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30512 relations extracted, e.g.\n",
      "AcroPhenRel(Span(\"(aBMD)\", parent=526044, chars=[276,281], words=[36,38]), Span(\"bone mineral density (aBMD)\", parent=526044, chars=[255,281], words=[33,38]))\n",
      "AcroPhenRel(Span(\"(aBMD)\", parent=526044, chars=[276,281], words=[36,38]), Span(\"mineral density (aBMD)\", parent=526044, chars=[260,281], words=[34,38]))\n",
      "AcroPhenRel(Span(\"(aBMD)\", parent=526044, chars=[276,281], words=[36,38]), Span(\"density (aBMD)\", parent=526044, chars=[268,281], words=[35,38]))\n",
      "AcroPhenRel(Span(\"(aBMD)\", parent=526044, chars=[276,281], words=[36,38]), Span(\"with areal bone mineral density (aBMD)\", parent=526044, chars=[244,281], words=[31,38]))\n",
      "AcroPhenRel(Span(\"(aBMD)\", parent=526044, chars=[276,281], words=[36,38]), Span(\"areal bone mineral density (aBMD)\", parent=526044, chars=[249,281], words=[32,38]))\n",
      "AcroPhenRel(Span(\"(GWA)\", parent=526044, chars=[198,202], words=[23,25]), Span(\"genome-wide association (GWA)\", parent=526044, chars=[174,202], words=[21,25]))\n",
      "AcroPhenRel(Span(\"(GWA)\", parent=526044, chars=[198,202], words=[23,25]), Span(\"Effects on Bone.Previous genome-wide association (GWA)\", parent=526044, chars=[149,202], words=[18,25]))\n",
      "AcroPhenRel(Span(\"(GWA)\", parent=526044, chars=[198,202], words=[23,25]), Span(\"association (GWA)\", parent=526044, chars=[186,202], words=[22,25]))\n",
      "AcroPhenRel(Span(\"(GWA)\", parent=526044, chars=[198,202], words=[23,25]), Span(\"on Bone.Previous genome-wide association (GWA)\", parent=526044, chars=[157,202], words=[19,25]))\n",
      "AcroPhenRel(Span(\"(GWA)\", parent=526044, chars=[198,202], words=[23,25]), Span(\"Bone.Previous genome-wide association (GWA)\", parent=526044, chars=[160,202], words=[20,25]))\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    txt_txt_rels = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Text Set5').one()\n",
    "except:\n",
    "    sentences = [s for doc in text_corpus for s in doc.sentences]\n",
    "    %time txt_txt_rels = ce3.extract(sentences, 'AcroPhenRel Text Set5', session)\n",
    "    \n",
    "print \"%s relations extracted, e.g.\" % len(txt_txt_rels)\n",
    "for cand in txt_txt_rels[:10]:\n",
    "    print cand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge all the candiates into a single set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37583 candidates in total\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    rels = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Canidates').one()\n",
    "except:\n",
    "    rels = CandidateSet(name='AcroPhenRel Canidates')\n",
    "    for c in tab_rels: rels.append(c)\n",
    "    for c in txt_tab_rels: rels.append(c)\n",
    "    for c in txt_txt_rels: rels.append(c)\n",
    "\n",
    "    session.add(rels)\n",
    "    session.commit()\n",
    "\n",
    "print '%d candidates in total' % len(rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down our results (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('acronyms.tmp.tsv', 'w') as f:\n",
    "    for rel in rels:\n",
    "        pmid = rel[0].parent.document.name\n",
    "        try:\n",
    "            out_str = '%s\\t%s\\t%s\\n' % (pmid, unicode(rel[1].get_span()), unicode(rel[0].get_span()))\n",
    "        except:\n",
    "            print (pmid, unicode(rel[1].get_span()), rel[0].get_span())\n",
    "        f.write(out_str.encode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a gold set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be helpful to have a list of gold labels against which to evaluate the accuracy of our system.\n",
    "\n",
    "We are going to load here a list of candidates that we have previously labeled by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotations = dict()\n",
    "with open('util/acronyms.anotated.txt') as f:\n",
    "    text = f.read()\n",
    "    for line in text.split('\\r'):\n",
    "        doc_id, str1, str2, res = line.strip().split('\\t')\n",
    "        res = 1 if int(res) == 1 else -1\n",
    "        annotations[(doc_id, str2, str1)] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of this file is: pmid, phenotype, acronym, label. We originally generated it from 100 random candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning the correctness of relations extracted from tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to use a machine learning classifier to identify correct acronyms amond our set of candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to train a classifier for candidates that have been extracted from tables (that had a phenotype and an acronym column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split data into an (unlabeled) training set (since we will use unsupervised risk estimation to train a candidate on it), and a dev/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 238 training and 288 dev/testing candidates\n",
      "Positive labels in dev/test set: 36\n",
      "Negative labels in dev/test set: 21\n"
     ]
    }
   ],
   "source": [
    "# helper fn\n",
    "def r2id(r):\n",
    "    doc_id = r[0].parent.document.name\n",
    "    str1, str2 = r[0].get_span(), r[1].get_span()\n",
    "    return (doc_id, str1, str2)\n",
    "\n",
    "try:\n",
    "    tab_train_c = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Table Training Candidates').one()\n",
    "    tab_devtest_c = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Table Dev/Test Candidates').one()\n",
    "except:\n",
    "    # delete any previous sets with that name\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Table Training Candidates').delete()\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Table Dev/Test Candidates').delete()\n",
    "\n",
    "    # helpers/config\n",
    "    frac_test = 0.5\n",
    "\n",
    "    # initialize the new sets\n",
    "    tab_train_c = CandidateSet(name='AcroPhenRel Table Training Candidates')\n",
    "    tab_devtest_c = CandidateSet(name='AcroPhenRel Table Dev/Test Candidates')\n",
    "\n",
    "    # choose a random subset for the labeled set\n",
    "    n_test = len(tab_rels) * frac_test\n",
    "    test_idx = set(np.random.choice(len(tab_rels), size=(n_test,), replace=False))\n",
    "\n",
    "    # add to the sets\n",
    "    for i, c in enumerate(tab_rels):\n",
    "        if i in test_idx:\n",
    "            tab_devtest_c.append(c)\n",
    "        elif r2id(c) in annotations:\n",
    "            tab_devtest_c.append(c)\n",
    "        else:\n",
    "            tab_train_c.append(c)\n",
    "\n",
    "    # save the results\n",
    "    session.add(tab_train_c)\n",
    "    session.add(tab_devtest_c)\n",
    "    session.commit()\n",
    "\n",
    "print 'Initialized %d training and %d dev/testing candidates' % (len(tab_train_c), len(tab_devtest_c))\n",
    "print \"Positive labels in dev/test set: %s\" % len([c for c in tab_devtest_c if annotations.get(r2id(c),0)==1])\n",
    "print \"Negative labels in dev/test set: %s\" % len([c for c in tab_devtest_c if annotations.get(r2id(c),0)==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the data programming approach, we define set of labeling functions. We will learn their accuracy via unsupervised learning and use them for classifying candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF1_digits(m):\n",
    "    txt = m[1].get_span()\n",
    "    frac_num = len([ch for ch in txt if ch.isdigit()]) / float(len(txt))\n",
    "    return -1 if frac_num > 0.5 else +1\n",
    "def LF1_short(m):\n",
    "    txt = m[1].get_span()\n",
    "    return -1 if len(txt) < 5 else 0\n",
    "\n",
    "LF_tables = [LF1_digits, LF1_short]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the LFs's on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 664 ms, sys: 54.8 ms, total: 718 ms\n",
      "Wall time: 867 ms\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "try:\n",
    "    %time L_tab_train = label_manager.load(session, tab_train_c, 'AcroPhenRel Table Training LF Labels')\n",
    "except sqlalchemy.orm.exc.NoResultFound:\n",
    "    %time L_tab_train = label_manager.create(session, tab_train_c, 'AcroPhenRel Table Training LF Labels', f=LF_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we learn their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t238\n",
      "Features:\t\t\t2\n",
      "================================================================================\n",
      "Begin training for rate=0.01, mu=1e-06\n",
      "\tLearning epoch = 0\tGradient mag. = 0.342480\n",
      "\tLearning epoch = 250\tGradient mag. = 0.182756\n",
      "\tLearning epoch = 500\tGradient mag. = 0.099294\n",
      "\tLearning epoch = 750\tGradient mag. = 0.057097\n",
      "\tLearning epoch = 1000\tGradient mag. = 0.041849\n",
      "\tLearning epoch = 1250\tGradient mag. = 0.053292\n",
      "\tLearning epoch = 1500\tGradient mag. = 0.088556\n",
      "\tLearning epoch = 1750\tGradient mag. = 0.148214\n",
      "\tLearning epoch = 2000\tGradient mag. = 0.226156\n",
      "\tLearning epoch = 2250\tGradient mag. = 0.281349\n",
      "\tLearning epoch = 2500\tGradient mag. = 0.262219\n",
      "\tLearning epoch = 2750\tGradient mag. = 0.190201\n",
      "\tLearning epoch = 3000\tGradient mag. = 0.118443\n",
      "\tLearning epoch = 3250\tGradient mag. = 0.067976\n",
      "\tLearning epoch = 3500\tGradient mag. = 0.037414\n",
      "\tLearning epoch = 3750\tGradient mag. = 0.020197\n",
      "\tLearning epoch = 4000\tGradient mag. = 0.010847\n",
      "\tLearning epoch = 4250\tGradient mag. = 0.005864\n",
      "\tLearning epoch = 4500\tGradient mag. = 0.003234\n",
      "\tLearning epoch = 4750\tGradient mag. = 0.001853\n",
      "\tLearning epoch = 5000\tGradient mag. = 0.001129\n",
      "\tLearning epoch = 5250\tGradient mag. = 0.000751\n",
      "\tLearning epoch = 5500\tGradient mag. = 0.000553\n",
      "\tLearning epoch = 5750\tGradient mag. = 0.000450\n",
      "\tLearning epoch = 6000\tGradient mag. = 0.000396\n",
      "\tLearning epoch = 6250\tGradient mag. = 0.000368\n",
      "\tLearning epoch = 6500\tGradient mag. = 0.000353\n",
      "\tLearning epoch = 6750\tGradient mag. = 0.000346\n",
      "\tLearning epoch = 7000\tGradient mag. = 0.000342\n",
      "\tLearning epoch = 7250\tGradient mag. = 0.000340\n",
      "\tLearning epoch = 7500\tGradient mag. = 0.000338\n",
      "\tLearning epoch = 7750\tGradient mag. = 0.000338\n",
      "\tLearning epoch = 8000\tGradient mag. = 0.000338\n",
      "\tLearning epoch = 8250\tGradient mag. = 0.000337\n",
      "\tLearning epoch = 8500\tGradient mag. = 0.000337\n",
      "\tLearning epoch = 8750\tGradient mag. = 0.000337\n",
      "\tLearning epoch = 9000\tGradient mag. = 0.000337\n",
      "\tLearning epoch = 9250\tGradient mag. = 0.000337\n",
      "\tLearning epoch = 9500\tGradient mag. = 0.000337\n",
      "\tLearning epoch = 9750\tGradient mag. = 0.000337\n",
      "Final gradient magnitude for rate=0.01, mu=1e-06: 0.000\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "tab_model = NaiveBayes()\n",
    "tab_model.train(L_tab_train, n_iter=10000, rate=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we classify the entire set of candidates. We start by applying the labelling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 830 ms, sys: 18.6 ms, total: 849 ms\n",
      "Wall time: 861 ms\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "try:\n",
    "    %time L_tab_all = label_manager.load(session, tab_rels, 'AcroPhenRel Table LF Labels')\n",
    "except sqlalchemy.orm.exc.NoResultFound:\n",
    "    %time L_tab_all = label_manager.create(session, tab_rels, 'AcroPhenRel Table LF Labels', f=LF_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model to predict which ones are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 262 acronyms predicted to be correct, e.g.\n",
      "[(u'24068947', u'MP:0000284', u'double outlet right ventricle'), (u'24068947', u'MP:0002625', u'heart left ventricle hypertrophy'), (u'24068947', u'MP:0009570', u'abnormal right lung morphology'), (u'24068947', u'MP:0002766', u'situs inversus'), (u'24068947', u'MP:0010429', u'abnormal heart left ventricle outflow tract morphology'), (u'24068947', u'MP:0000531', u'right pulmonary isomerism'), (u'24068947', u'MP:0000276', u'heart right ventricle hypertrophy'), (u'24068947', u'MP:0000542', u'left-sided isomerism'), (u'24068947', u'MP:0003922', u'abnormal heart right atrium morphology'), (u'24068947', u'MP:0009569', u'abnormal left lung morphology')]\n"
     ]
    }
   ],
   "source": [
    "scores = tab_model.odds_unw(L_tab_all)\n",
    "tab_acronyms = [r2id(c) for (c, s) in zip(tab_rels, scores) if s > 0]\n",
    "\n",
    "print 'Identified %d acronyms predicted to be correct, e.g.' % len(tab_acronyms)\n",
    "print tab_acronyms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the correctness of relations extracted from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we repeat our classification procedure on relations that have been extracted from phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the set of all phrase relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 37057 candidates from phrases\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "try:\n",
    "    txt_rels = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Text Canidates').one()\n",
    "except:\n",
    "    txt_rels = CandidateSet(name='AcroPhenRel Text Canidates')\n",
    "    for c in txt_tab_rels: txt_rels.append(c)\n",
    "    for c in txt_txt_rels: txt_rels.append(c)\n",
    "\n",
    "    session.add(txt_rels)\n",
    "    session.commit()\n",
    "\n",
    "print 'Collected %d candidates from phrases' % len(txt_rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split data into an (unlabeled) training set (since we will use unsupervised risk estimation to train a candidate on it), and a dev/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized 18456 training and 18601 dev/testing candidates\n",
      "Positive labels in dev/test set: 28\n",
      "Negative labels in dev/test set: 125\n"
     ]
    }
   ],
   "source": [
    "# helper\n",
    "def r2id(r):\n",
    "    doc_id = r[0].parent.document.name\n",
    "    str1, str2 = r[0].get_span(), r[1].get_span()\n",
    "    return (doc_id, str1, str2)\n",
    "    \n",
    "try:\n",
    "    txt_train_c = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Phrase Training Candidates').one()\n",
    "    txt_devtest_c = session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Phrase Dev/Test Candidates').one()\n",
    "except:\n",
    "    # delete any previous sets with that name\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Phrase Training Candidates').delete()\n",
    "    session.query(CandidateSet).filter(CandidateSet.name == 'AcroPhenRel Phrase Dev/Test Candidates').delete()\n",
    "\n",
    "    # helpers/config\n",
    "    frac_test = 0.5\n",
    "\n",
    "    # initialize the new sets\n",
    "    txt_train_c = CandidateSet(name='AcroPhenRel Phrase Training Candidates')\n",
    "    txt_devtest_c = CandidateSet(name='AcroPhenRel Phrase Dev/Test Candidates')\n",
    "\n",
    "    # choose a random subset for the labeled set\n",
    "    n_test = len(txt_rels) * frac_test\n",
    "    test_idx = set(np.random.choice(len(txt_rels), size=(n_test,), replace=False))\n",
    "\n",
    "    # add to the sets\n",
    "    for i, c in enumerate(txt_rels):\n",
    "        if i in test_idx:\n",
    "            txt_devtest_c.append(c)\n",
    "        elif r2id(c) in annotations:\n",
    "            txt_devtest_c.append(c)\n",
    "        else:\n",
    "            txt_train_c.append(c)\n",
    "\n",
    "    # save the results\n",
    "    session.add(txt_train_c)\n",
    "    session.add(txt_devtest_c)\n",
    "    session.commit()\n",
    "\n",
    "print 'Initialized %d training and %d dev/testing candidates' % (len(txt_train_c), len(txt_devtest_c))\n",
    "print \"Positive labels in dev/test set: %s\" % len([c for c in txt_devtest_c if annotations.get(r2id(c),0)==1])\n",
    "print \"Negative labels in dev/test set: %s\" % len([c for c in txt_devtest_c if annotations.get(r2id(c),0)==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the data programming approach, we define set of labeling functions. We will learn their accuracy via unsupervised learning and use them for classifying candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from snorkel.lf_helpers import get_left_tokens, left_text\n",
    "\n",
    "# helper fn\n",
    "def r2id(r):\n",
    "    doc_id = r[0].parent.document.name\n",
    "    str1, str2 = r[0].get_span(), r[1].get_span()\n",
    "    acro = str1[1:-1]\n",
    "    phen = str2.split(' (')[0]\n",
    "    return (doc_id, acro, phen)\n",
    "\n",
    "# positive LFs\n",
    "def LF_acro_matches(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    words = phen.strip().split()\n",
    "    if len(acro) == len(words):\n",
    "        w_acro = ''.join([w[0] for w in words])\n",
    "        if w_acro.lower() == acro.lower():\n",
    "            return +1\n",
    "    return 0\n",
    "\n",
    "def LF_acro_matches_with_dashes(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    words = re.split(' |-', phen)\n",
    "    if len(acro) == len(words) and len(words) > 0:\n",
    "        w_acro = ''.join([w[0] for w in words if w])\n",
    "        if w_acro.lower() == acro.lower():\n",
    "            return +1\n",
    "    return 0\n",
    "\n",
    "def LF_acro_first_letter(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    if not any(l.islower() for l in phen): return 0\n",
    "    words = phen.strip().split()\n",
    "    if len(acro) <= len(words):\n",
    "        if words[0].lower() == acro[0].lower():\n",
    "            return +1\n",
    "    return 0\n",
    "\n",
    "def LF_acro_prefix(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    phen = phen.replace('-', '')\n",
    "    if phen[:2].lower() == acro[:2].lower():\n",
    "        return +5\n",
    "    return 0\n",
    "\n",
    "def LF_acro_matches_last_letters(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    words = phen.strip().split()\n",
    "#     prev_words = m.span1.pre_window(d=1) + words\n",
    "    prev_words = left_text(m[1], window=1) + words\n",
    "    w_prev_acro = ''.join([w[0] for w in prev_words])\n",
    "    if w_prev_acro.lower() == acro.lower(): return 0\n",
    "    for r in (1,2):\n",
    "        new_acro = acro[r:]\n",
    "        if len(new_acro) < 3: continue\n",
    "        if len(new_acro) == len(words):\n",
    "            w_acro = ''.join([w[0] for w in words])\n",
    "            if w_acro.lower() == new_acro.lower():\n",
    "                return +1\n",
    "    return 0\n",
    "\n",
    "def LF_full_cell(m):\n",
    "    \"\"\"If only phrase in cell is A B C (XYZ), then it's correct\"\"\"\n",
    "    if not hasattr(m[1].parent, 'cell'): return 0\n",
    "    _, acro, phen = r2id(m)\n",
    "#     if not phen[0].lower() == acro[0].lower(): return 0\n",
    "    cell = m[1].parent.cell\n",
    "    txt_cell = soup(cell.text).text if cell.text is not None else ''\n",
    "    txt_span = m[1].get_span()\n",
    "    return 1 if cell.text == txt_span or txt_cell == txt_span else 0\n",
    "#     return 1 if m[1].parent.cell.text == m[1].get_span() else 0\n",
    "\n",
    "def LF_start(m):\n",
    "    punc = ',.;!?()\\'\"'\n",
    "    if hasattr(m[1].parent, 'cell'): return 0 # this is only for when we're within a sentence\n",
    "    if m[1].get_word_start() == 0 or any(c in punc for c in left_text(m[1], window=1)):\n",
    "        _, acro, phen = r2id(m)\n",
    "        if phen[0].lower() == acro[0].lower(): \n",
    "            return +1\n",
    "    return 0\n",
    "\n",
    "LF_txt_pos = [LF_acro_matches, LF_acro_matches_with_dashes, LF_acro_first_letter, LF_acro_prefix, LF_acro_matches_last_letters, LF_full_cell, LF_start]\n",
    "\n",
    "# negative LFs\n",
    "def LF_digits(m):\n",
    "    txt = m[1].get_span()\n",
    "    frac_num = len([ch for ch in txt if ch.isdigit()]) / float(len(txt))\n",
    "    return -1 if frac_num > 0.5 else +1\n",
    "\n",
    "def LF_short(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    return -1 if len(acro) == 1 else 0\n",
    "\n",
    "def LF_lc(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    return -1 if all(l.islower() for l in acro) else 0\n",
    "\n",
    "def LF_uc(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    return -2 if not any(l.islower() for l in phen) else 0\n",
    "\n",
    "def LF_punc(m):\n",
    "    _, acro, phen = r2id(m)\n",
    "    punc = ',.;!?()'\n",
    "    return -1 if any(c in punc for c in phen) else 0\n",
    "    \n",
    "\n",
    "LF_txt_neg = [LF_digits, LF_short, LF_lc, LF_uc, LF_punc]\n",
    "\n",
    "LF_txt = LF_txt_pos + LF_txt_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the LFs on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating annotations for 18456 candidates...\n",
      "[========================================] 100%\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 4min 57s, sys: 2.51 s, total: 5min\n",
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "try:\n",
    "    %time L_txt_train = label_manager.load(session, txt_train_c, 'AcroPhenRel Phrase Training LF Labels k')\n",
    "except sqlalchemy.orm.exc.NoResultFound:\n",
    "    %time L_txt_train = label_manager.create(session, txt_train_c, 'AcroPhenRel Phrase Training LF Labels k', f=LF_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we learn their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training marginals (!= 0.5):\t18456\n",
      "Features:\t\t\t12\n",
      "================================================================================\n",
      "Begin training for rate=0.1, mu=1e-06\n",
      "\tLearning epoch = 0\tGradient mag. = 0.297661\n",
      "\tLearning epoch = 250\tGradient mag. = 0.179091\n",
      "\tLearning epoch = 500\tGradient mag. = 0.080737\n",
      "\tLearning epoch = 750\tGradient mag. = 0.041660\n",
      "\tLearning epoch = 1000\tGradient mag. = 0.022234\n",
      "\tLearning epoch = 1250\tGradient mag. = 0.013254\n",
      "\tLearning epoch = 1500\tGradient mag. = 0.009804\n",
      "\tLearning epoch = 1750\tGradient mag. = 0.008692\n",
      "\tLearning epoch = 2000\tGradient mag. = 0.008353\n",
      "\tLearning epoch = 2250\tGradient mag. = 0.008261\n",
      "\tLearning epoch = 2500\tGradient mag. = 0.008269\n",
      "\tLearning epoch = 2750\tGradient mag. = 0.008336\n",
      "\tLearning epoch = 3000\tGradient mag. = 0.008454\n",
      "\tLearning epoch = 3250\tGradient mag. = 0.008627\n",
      "\tLearning epoch = 3500\tGradient mag. = 0.008874\n",
      "\tLearning epoch = 3750\tGradient mag. = 0.009231\n",
      "\tLearning epoch = 4000\tGradient mag. = 0.009771\n",
      "\tLearning epoch = 4250\tGradient mag. = 0.010644\n",
      "\tLearning epoch = 4500\tGradient mag. = 0.012181\n",
      "\tLearning epoch = 4750\tGradient mag. = 0.015199\n",
      "\tLearning epoch = 5000\tGradient mag. = 0.022055\n",
      "\tLearning epoch = 5250\tGradient mag. = 0.040339\n",
      "\tLearning epoch = 5500\tGradient mag. = 0.069061\n",
      "\tLearning epoch = 5750\tGradient mag. = 0.054091\n",
      "\tLearning epoch = 6000\tGradient mag. = 0.031657\n",
      "\tLearning epoch = 6250\tGradient mag. = 0.020682\n",
      "\tLearning epoch = 6500\tGradient mag. = 0.016568\n",
      "\tLearning epoch = 6750\tGradient mag. = 0.015236\n",
      "\tLearning epoch = 7000\tGradient mag. = 0.014816\n",
      "\tLearning epoch = 7250\tGradient mag. = 0.014676\n",
      "\tLearning epoch = 7500\tGradient mag. = 0.014625\n",
      "\tLearning epoch = 7750\tGradient mag. = 0.014603\n",
      "\tLearning epoch = 8000\tGradient mag. = 0.014594\n",
      "\tLearning epoch = 8250\tGradient mag. = 0.014589\n",
      "\tLearning epoch = 8500\tGradient mag. = 0.014587\n",
      "\tLearning epoch = 8750\tGradient mag. = 0.013926\n",
      "\tLearning epoch = 9000\tGradient mag. = 0.011601\n",
      "\tLearning epoch = 9250\tGradient mag. = 0.009668\n",
      "\tLearning epoch = 9500\tGradient mag. = 0.008061\n",
      "\tLearning epoch = 9750\tGradient mag. = 0.006725\n",
      "Final gradient magnitude for rate=0.1, mu=1e-06: 0.006\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "txt_model = NaiveBayes()\n",
    "txt_model.train(L_txt_train, n_iter=10000, rate=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we classify the entire set of candidates. We start by applying the labelling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating annotations for 37057 candidates...\n",
      "[========================================] 100%\n",
      "Loading sparse Label matrix...\n",
      "CPU times: user 9min 37s, sys: 4.64 s, total: 9min 42s\n",
      "Wall time: 9min 33s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "label_manager = LabelManager()\n",
    "\n",
    "try:\n",
    "    %time L_txt_all = label_manager.load(session, txt_rels, 'AcroPhenRel Table LF Labels f')\n",
    "except sqlalchemy.orm.exc.NoResultFound:\n",
    "    %time L_txt_all = label_manager.create(session, txt_rels, 'AcroPhenRel Table LF Labels f', f=LF_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model to predict which ones are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 24544 acronyms predicted to be correct, e.g.\n",
      "[(u'24465473', u'SE', u'Beta'), (u'24465473', u'SE', u'Beta'), (u'24465473', u'SE', u'Beta'), (u'23958962', u'SNFs', u'GWAS Sample'), (u'23958962', u'SNFs', u'Sample'), (u'23958962', u'Unrelateds', u'GWAS Sample'), (u'23958962', u'Unrelateds', u'Sample'), (u'23958962', u'Unrelateds', u'Sample'), (u'23958962', u'Unrelateds', u'Replication Sample'), (u'19081515', u'vCJD', u'p')]\n"
     ]
    }
   ],
   "source": [
    "scores = txt_model.odds_unw(L_txt_all)\n",
    "txt_acronyms = [r2id(c) for (c, s) in zip(txt_rels, scores) if s > 0]\n",
    "\n",
    "print 'Identified %d acronyms predicted to be correct, e.g.' % len(txt_acronyms)\n",
    "print txt_acronyms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Store the predicted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4451 acronyms resolved\n"
     ]
    }
   ],
   "source": [
    "acronyms = tab_acronyms + txt_acronyms\n",
    "print '%d acronyms resolved' % len(acronyms)\n",
    "\n",
    "# store relations to annotate\n",
    "with open('results/nb-output/acronyms.extracted.all.tsv', 'w') as f:\n",
    "    for doc_id, str1, str2 in acronyms:\n",
    "        if doc_id.endswith('-doc'): doc_id = doc_id[:-4]\n",
    "        try:\n",
    "            out = u'{}\\t{}\\t{}\\n'.format(doc_id, unicode(str2), str1)\n",
    "            f.write(out.encode(\"UTF-8\"))\n",
    "        except:\n",
    "            print 'ERROR:', str1, str2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
